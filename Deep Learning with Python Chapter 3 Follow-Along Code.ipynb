{"cells":[{"source":["# Downloading/loading the built-in imdb data\n","from keras.datasets import imdb\n","\n","#Setting up train and test data\n","(trainData, trainLabels), (testData, testLabels) = imdb.load_data(\n","    num_words = 10000 #Only keep top 10K words\n",")\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["#Decoding one of the reviews back to English, just to see how it's done\n","wordIndex = imdb.get_word_index()\n","reverseWordIndex = dict(\n","    [(value, key) for (key, value) in wordIndex.items()])\n","decodedReview = ' '.join(\n","    [reverseWordIndex.get(i - 3, '?') for i in trainData[0]])\n","print(decodedReview)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["# Preparing the data\n","\"You can't feed a list of integers into a neural network. You have to turn them\n","into into tensors. There are two ways to do that:<br/><br/>\n","1. Pad your lists so they all have the same length, turn them into n integer\n","tensor of shape (samples, word_indices), and then use as the first layer in\n","your network-- a layer capable of handling such integer tensors (the 'embedding'\n","layer, covered later in the book).<br/><br/>\n","2. One-hot encode your lists to turn them into vectors of 0s and 1s. This would mean,\n","for instance, turning the sequence [3, 5] into a 10K-dimensional vector\n","that would all be zeroes except for indices 3 and 5, which would be ones. Then you\n","could use as the first layer in your network a 'Dense' layer, capable of handling\n","floating-point vector data\n","\n","Let's go with the latter solution to vectorize the data, which you;ll do manually for\n","maximum clarity\""],"metadata":{}},{"source":["import numpy as np \n","\n","# Create an all-zero matrix of shape(len(sequences), dimension)\n","def vectorizeSequences(sequences, dimension = 10000):\n","    results = np.zeroes((len(sequences), dimension))\n","    for i, sequence in enumerate(sequences):\n","        #Set specific indices of results[i] to 1s\n","        results[i, sequence] = 1.\n","    return results\n","\n","    xTrain = vectorizeSequences(trainData) #Vectorized training data\n","    xTest = vectorizeSequences(testData) #Vectorized test data\n","\n","    #Vectorize the labels, as well\n","    yTrain = np.asarray(trainLabels).astype('float32')\n","    yTest = np.asarray(testLabels).astype('float32')\n","\n","\n","    "],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["# Building Your Network\n","\"The input data are vectors, and the labels are scalars.\"\n","This type of data works well with a simple stack of fully connected ('Dense')\n","layers with 'relu' activations : Dense(16, activation = 'relu')<br/>\n","The above line passes 16 to the Dense layer because that's the\n","number of 'hidden units' in the layer. <br/><br/>\n","Hidden Units = dimension in the representation space\n","of the layer. A way of thinking about hidden units is that they\n","represent \"how much freedom you're allowing the representation to\n","have when learning internal representations.\" As hidden units (dimensional\n","representation space) increases, your model can handle higher-complexity\n","problems, but computational complexity goes up, as does the potential\n","for overfitting.\n","### Two Key Architecture Decisions - How many layers to use and\n","### how many hidden units per layer\n","(We'll cover how to do this in the next chapter. For now, he chooses for us)<br/><br/>\n","For this chapter, we'll use this architecture:\n","* Two intermediate layers with 16 hidden units each\n","* A third, output layer that will return the scalar prediction\n","Relu activation will be used on the intermediate layers, and we'll use signmoid\n","activation on the output layer so that we get probability scores<br/><br/>\n","*Note: Relu (rectified linear unit) zeroes out negative numbers*\n","# The Model Definition in Keras"],"metadata":{}},{"source":["from keras import models\n","from keras import layers\n","\n","model = models.Sequential()\n","model.add(layers.Dense(16, activation = 'relu', input_shape = (10000, )))\n","model.add(layers.Dense(16, activation = 'relu'))\n","model.add(layers.Dense(1, activation = 'sigmoid'))\n","\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":[""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}